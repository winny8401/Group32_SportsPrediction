# -*- coding: utf-8 -*-
"""Group32__SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vl4cZd2JfkeC5m8VDgPN6_KjUDi1OKrm
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
import random as rnd
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

"""**QUESTION 1**

**Demonstrate the data preparation & feature extraction process**

*Demonstrated removing useless varaiables, EDA, imputation and encoding.*
"""

#Loading  the datasets
players_21 = pd.read_csv('/content/drive/MyDrive/players_21.csv')
players_22 = pd.read_csv('/content/drive/MyDrive/players_22.csv')

players_21.head(5)

players_22.head(5)

"""Data Pre-processing

```


"""

# Substring to remove
substring_to_remove = 'url'

# Remove columns with specified substring
players_21 = players_21[players_21.columns.drop(players_21.filter(like=substring_to_remove))]

# Check the shape of the DataFrame
print(players_21.shape)
players_21.head(5)

# Substring to remove
substring_to_remove = 'url'

# Remove columns with specified substring
players_22 = players_22[players_22.columns.drop(players_22.filter(like=substring_to_remove))]

# Check the shape of the DataFrame
print(players_22.shape)
players_22.head(5)

players_21.dtypes

players_22.dtypes

# Creating  an empty list to store the names of columns with more than 50% missing data (for players_21 dataset)
cols_to_drop = []

# Iterate through each column in the DataFrame
for column_name in players_21.columns:
    # Calculate the percentage of missing data in the current column
    missing_percentage = (players_21[column_name].isnull().sum() / len(players_21)) * 100

    # Check if the missing percentage is greater than 50%
    if missing_percentage > 50:
        # Print the column name and its missing percentage
        print(f'{column_name} - {missing_percentage:.2f}%')

        # Add the column name to the list of columns to drop
        cols_to_drop.append(column_name)
print(cols_to_drop)

# Drop the specified columns from the DataFrame
players_21.drop(columns=cols_to_drop, inplace=True)

# Print the new shape of the DataFrame
print(players_21.shape)

# Display the first 5 rows of the DataFrame
players_21.head()

# Creating  an empty list to store the names of columns with more than 50% missing data (for players_22 dataset)
cols1_to_drop = []

# Iterate through each column in the DataFrame
for column_name in players_22.columns:
    # Calculate the percentage of missing data in the current column
    missing_percentage = (players_22[column_name].isnull().sum() / len(players_22)) * 100

    # Check if the missing percentage is greater than 50%
    if missing_percentage > 50:
        # Print the column name and its missing percentage
        print(f'{column_name} - {missing_percentage:.2f}%')

        # Add the column name to the list of columns to drop
        cols1_to_drop.append(column_name)

# Drop the specified columns from the DataFrame
players_22.drop(columns=cols_to_drop, inplace=True)

# Print the new shape of the DataFrame
print(players_22.shape)

# Display the first 5 rows of the DataFrame
players_22.head()

# Create a list of column patterns you want to keep (for players_21 dataset)
column_patterns_to_keep = ['sofifa_id', 'skill_moves', 'movement_', 'defending_', 'goalkeeping_', 'attacking_', 'power_', 'mentality_']

# Create an empty list to store the columns to keep
columns_to_keep = []

# Loop through the list of column patterns
for pattern in column_patterns_to_keep:
    # Check each column in the DataFrame
    for column in players_21.columns:
        # If the pattern is found in the column name, keep the column
        if pattern in column:
            columns_to_keep.append(column)

# Filter the DataFrame to keep only the selected columns
players_21_filtered = players_21[columns_to_keep]

# Print the shape of the filtered DataFrame
print("Shape of the filtered DataFrame:", players_21_filtered.shape)
players_21_filtered.head()

# Create a list of column patterns you want to keep (for players_22 dataset)
column1_patterns_to_keep = ['sofifa_id', 'skill_moves', 'movement_', 'defending_', 'goalkeeping_', 'attacking_', 'power_', 'mentality_']

# Create an empty list to store the columns to keep
columns1_to_keep = []

# Loop through the list of column patterns
for pattern in column1_patterns_to_keep:
    # Check each column in the DataFrame
    for column in players_22.columns:
        # If the pattern is found in the column name, keep the column
        if pattern in column:
            columns1_to_keep.append(column)

# Filter the DataFrame to keep only the selected columns
players_22_filtered = players_22[columns_to_keep]

# Print the shape of the filtered DataFrame
print("Shape of the filtered DataFrame:", players_22_filtered.shape)
players_22_filtered.head()

players_21_filtered.columns

players_22_filtered.columns

columns_to_remove = ['sofifa_id', 'short_name', 'long_name', 'player_positions', 'dob', 'club_name', 'nationality_name']
players_21_filtered = players_21.drop(columns=columns_to_remove)
players_21_filtered.head()

columns_to_remove = ['sofifa_id', 'short_name', 'long_name', 'player_positions', 'dob', 'club_name', 'nationality_name']
players_22_filtered = players_22.drop(columns=columns_to_remove)
players_22_filtered.head()

# Summary statistics for players_21_filtered
summary = players_21_filtered.describe()

# Distribution of 'overall' ratings
plt.hist(players_21_filtered['overall'], bins=20, edgecolor='k')
plt.title('Distribution of Player Overall Ratings')
plt.xlabel('Overall Rating')
plt.ylabel('Count')
plt.show()

#Summary statistics
summary = players_21.describe()

#Distribution of 'overall' ratings
plt.hist(players_21['overall'], bins=20, edgecolor='k')
plt.title('Distribution of Player Overall Ratings')
plt.xlabel('Overall Rating')
plt.ylabel('Count')
plt.show()

# Impute missing values for numeric columns with the median
numeric_columns = players_21_filtered.select_dtypes(include='number')
players_21_filtered[numeric_columns.columns] = numeric_columns.fillna(numeric_columns.median())

# Impute missing values for categorical columns with the mode
categorical_columns = players_21_filtered.select_dtypes(exclude='number')
players_21_filtered[categorical_columns.columns] = categorical_columns.fillna(categorical_columns.mode().iloc[0])
(players_21_filtered)

# Impute missing values for numeric columns with the median
numeric_columns = players_22_filtered.select_dtypes(include='number')
players_22_filtered[numeric_columns.columns] = numeric_columns.fillna(numeric_columns.median())

# Impute missing values for categorical columns with the mode
categorical_columns = players_22_filtered.select_dtypes(exclude='number')
players_22_filtered[categorical_columns.columns] = categorical_columns.fillna(categorical_columns.mode().iloc[0])
(players_22_filtered)

# One-hot encoding for 'club_position'
players_21_filtered = pd.get_dummies(players_21_filtered, columns=['club_position'], prefix='position')

# Label encoding for 'preferred_foot'
players_21_filtered['preferred_foot'] = players_21_filtered['preferred_foot'].map({'Right': 0, 'Left': 1})

# Label encoding for 'work_rate'
players_21_filtered['work_rate'] = players_21_filtered['work_rate'].astype('category').cat.codes

# Print the transformed DataFrame
players_21_filtered

# One-hot encoding for 'club_position'
players_22_filtered = pd.get_dummies(players_22_filtered, columns=['club_position'], prefix='position')

# Label encoding for 'preferred_foot'
players_22_filtered['preferred_foot'] = players_22_filtered['preferred_foot'].map({'Right': 0, 'Left': 1})

# Label encoding for 'work_rate'
players_22_filtered['work_rate'] = players_22_filtered['work_rate'].astype('category').cat.codes

# Print the transformed DataFrame
players_22_filtered

"""**QUESTION 2**

**Create feature subsets that show maximum correlation with the dependent variable.**

*Created feature subsets which show better correlation with the overall rating and scaled the independent variables.*

Feature Engineering- Creating feature subsets and finding the maximum correlation
"""

# Calculate the correlation matrix (for players_22 dataset)
correlation_matrix_21 = players_21_filtered.corr()

# Number of top features to select
N = 20

# Getting the top N features with the highest absolute correlation with 'overall'
top_correlated_features = correlation_matrix_21['overall'].abs().nlargest(N + 1).index

# Remove 'overall' from the list of top correlated features
top_correlated_features = top_correlated_features.drop('overall')

# Creating a feature subset with the selected top features (the independent variables)
feature_subset_21 = players_21_filtered[top_correlated_features]

# Define the dependent variable
dependent_variable_21 = players_21_filtered['overall']

# Display the first few rows of feature_subset_21
feature_subset_21

dependent_variable_21

# Calculate the correlation matrix (for players_22 dataset)
correlation_matrix_22 = players_22_filtered.corr()

# Number of top features to select
N = 20

# Getting the top N features with the highest absolute correlation with 'overall'
top_correlated_features = correlation_matrix_22['overall'].abs().nlargest(N + 1).index

# Remove 'overall' from the list of top correlated features
top_correlated_features = top_correlated_features.drop('overall')

# Creating a feature subset with the selected top features (the independent variables)
feature_subset_22 = players_22_filtered[top_correlated_features]

# Define the dependent variable
dependent_variable_22 = players_22_filtered['overall']

feature_subset_22

dependent_variable_22

#Scaling independent variables
columns_to_scale = [ 'movement_reactions', 'mentality_composure', 'passing', 'potential',
                    'release_clause_eur', 'dribbling', 'wage_eur', 'power_shot_power', 'value_eur',
                    'mentality_vision', 'attacking_short_passing', 'physic', 'skill_long_passing', 'age',
                    'shooting', 'skill_ball_control', 'international_reputation', 'skill_curve', 'attacking_crossing']

scaled_data = feature_subset_21[columns_to_scale]

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the selected columns
scaled_data = scaler.fit_transform(scaled_data)


  # Replace the original columns with the scaled data in the DataFrame
feature_subset_21[columns_to_scale] = scaled_data

"""**QUESTION 3**

**Create and train a suitable machine learning model with cross-validation that can predict a player's rating.**

*Created and trained with cross-validation either RandomForest, XGBoost, Gradient Boost Regressors that can predict a player rating.*
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(feature_subset_21, players_21_filtered['overall'], test_size=0.2, random_state=42)


# Train a Random Forest Regressor
random_forest = RandomForestRegressor(random_state=42)
random_forest.fit(X_train, y_train)
rf_predictions = random_forest.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

print("Random Forest MSE:", rf_mse)
print("Random Forest R-squared:", rf_r2)

# Train an XGBoost Regressor
xgboost = XGBRegressor(random_state=42)
xgboost.fit(X_train, y_train)
xgb_predictions = xgboost.predict(X_test)
xgb_mse = mean_squared_error(y_test, xgb_predictions)
xgb_r2 = r2_score(y_test, xgb_predictions)

print("XGBoost MSE:", xgb_mse)
print("XGBoost R-squared:", xgb_r2)

# Train a Gradient Boosting Regressor
gradient_boosting = GradientBoostingRegressor(random_state=42)
gradient_boosting.fit(X_train, y_train)
gb_predictions = gradient_boosting.predict(X_test)
gb_mse = mean_squared_error(y_test, gb_predictions)
gb_r2 = r2_score(y_test, gb_predictions)

print("Gradient Boosting MSE:", gb_mse)
print("Gradient Boosting R-squared:", gb_r2)

"""**Question 4**

**Measure the model's performance and fine-tune it as a process of optimization.**

*Used MAE or RMSE and then fine tuned model, train and tested it again.*
"""

rf_mae = mean_absolute_error(y_test, rf_predictions)
xgb_mae = mean_absolute_error(y_test, xgb_predictions)
gb_mae = mean_absolute_error(y_test, gb_predictions)

print("Random Forest MAE:", rf_mae)
print("XGBoost MAE:", xgb_mae)
print("Gradient Boosting MAE:", gb_mae)

# Define parameter grids for hyperparameter tuning
rf_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20]}
xgb_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5]}
gb_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5]}

# Hyperparameter tuning for each model
# Random Forest
rf_grid_search = GridSearchCV(random_forest, param_grid=rf_param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
rf_grid_search.fit(X_train, y_train)
best_rf_model = rf_grid_search.best_estimator_
best_rf_predictions = best_rf_model.predict(X_test)
best_rf_mae = mean_absolute_error(y_test, best_rf_predictions)
print("Best Random Forest MAE:", best_rf_mae)

# XGBoost
xgb_grid_search = GridSearchCV(xgboost, param_grid=xgb_param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
xgb_grid_search.fit(X_train, y_train)
best_xgb_model = xgb_grid_search.best_estimator_
best_xgb_predictions = best_xgb_model.predict(X_test)
best_xgb_mae = mean_absolute_error(y_test, best_xgb_predictions)
print("Best XGBoost MAE:", best_xgb_mae)

# Gradient Boosting
gb_grid_search = GridSearchCV(gradient_boosting, param_grid=gb_param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
gb_grid_search.fit(X_train, y_train)
best_gb_model = gb_grid_search.best_estimator_
best_gb_predictions = best_gb_model.predict(X_test)
best_gb_mae = mean_absolute_error(y_test, best_gb_predictions)
print("Best Gradient Boosting MAE:", best_gb_mae)

# Creating an ensemble model using VotingRegressor
ensemble_model = VotingRegressor(estimators=[
    ('RandomForest', best_rf_model),
    ('XGBoost', best_xgb_model),
    ('GradientBoosting', best_gb_model)
])

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(feature_subset_21, players_21_filtered['overall'], test_size=0.2, random_state=42)

# Train the ensemble model
ensemble_model.fit(X_train, y_train)

# Making predictions with the ensemble model
ensemble_predictions = ensemble_model.predict(X_test)

# Calculate the mean absolute error for the ensemble model
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)

print("Ensemble Model Mean Absolute Error:", ensemble_mae)

"""**Question 5**


**Use the data from another season(players_22) which was not used during the training to test how good is the model.**


*Used the data(players_22) to test how good is the model with completely new data.*
"""

#Testing with the gradient boosting model
predictions_players_22 = best_rf_model.predict(X_test)

players_22_mae = mean_absolute_error(y_test,predictions_players_22)
print("MAE on players_22 dataset:", players_22_mae)

from sklearn.ensemble import VotingRegressor

# Create an ensemble model using VotingRegressor
ensemble_model = VotingRegressor(estimators=[
    ('RandomForest', best_rf_model),
    ('XGBoost', best_xgb_model),
    ('GradientBoosting', best_gb_model)
])

# Fit the ensemble model on the training data (X_train, y_train)
ensemble_model.fit(X_train, y_train)

# Use the ensemble model to predict player ratings for players_22 (X_test)
ensemble_predictions_players_22 = ensemble_model.predict(X_test)

# Calculate the mean absolute error for the ensemble model on players_22 (y_test)
ensemble_players_22_mae = mean_absolute_error(y_test, ensemble_predictions_players_22)

print("MAE on players_22 dataset using Ensemble Model:", ensemble_players_22_mae)

import pickle
import joblib

# Save the model using joblib
joblib.dump(best_rf_model, 'FIFA_football_prediction_model.pkl')

